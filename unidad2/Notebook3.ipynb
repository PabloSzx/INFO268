{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unidad 2: Taller de análisis de datos textuales\n",
    "\n",
    "<h1> Notebook 3 - Introducción a la vectorización de palabras: Word2Vec</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los notebooks anteriores, hemos hablado de los vectores que se utilizan para representar nuestros datos, textuales o no, en una forma matemática, y así poder aplicar métodos de machine learning.\n",
    "\n",
    "En este notebook, vamos a llevar esta idea un paso más allá y generar reprentaciones vectoriales de las <u>palabras</u>. Esta idea es generalmente llamada __Word Embeddings__. __Word2Vec__ es un algoritmo popular para implementar esta idea.\n",
    "Esta técnica permite para representar mejor el significado de una palabra.\n",
    "\n",
    "Para más detalles sobre los fundamentos de Word2Vec, se puede leer: _Efficient Estimation of Word Representations in Vector Space_ [[Mikolov et al., 2013]](https://arxiv.org/pdf/1301.3781.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es una representación vectorial de una palabra?\n",
    "\n",
    "Supongamos que nuestro vocabulario tiene sólo cinco palabras: King, Queen, Man, Woman y Child. Podríamos codificar la palabra 'Queen' como:\n",
    "\n",
    "<img src=\"word2vec1.png\"/>\n",
    "\n",
    "Usando tal codificación, no hay comparación interesante que podamos hacer entre vectores de palabras. Con word2vec, se utiliza una representación distribuida de una palabra. Cada palabra está representada por una distribución de pesos entre esos elementos. La representación de una palabra se extiende a través de todos los elementos en el vector, y cada elemento en el vector contribuye a la definición de muchas palabras.\n",
    "\n",
    "<img src=\"word2vec2.png\"/>\n",
    "\n",
    "Tal vector llega a representar de alguna manera abstracta el 'significado' de una palabra. Y como veremos a continuación, simplemente examinando un corpus grande es posible aprender vectores de palabras que son capaces de capturar las relaciones entre palabras de una manera sorprendentemente expresiva. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Razonamiento con vectores de palabras\n",
    "\n",
    "Los vectores son muy buenos para responder a problemas de analogía de tipo: '_A_ es a _B_ lo que _C_ es a ...?'. Por ejemplo, el _hombre_ es a la _mujer_ lo que el _tío_ es a la _tía_ usando un método de desplazamiento vectorial.\n",
    "\n",
    "Por ejemplo, el desplazamiento vectorial que ilustra la _relación de género_:\n",
    "\n",
    "<img src=\"word2vec3.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de composición vectorial también nos permite responder a la pregunta __King - Man + Woman = ?__ y llegar al resultado __Queen__.\n",
    "\n",
    "<img src=\"word2vec4.png\"/>\n",
    "\n",
    "Estos modelos vectoriales de palabras están destacables ya que permiten representar ciertas relaciones entre palabras sin aportar información sobre la semántica de las palabras. Como vamos a verlo, se pueden aprender los vectores a partir de gran conjuntos de datos textuales.\n",
    "\n",
    "Algunos ejemples de relaciones entre palabras que se han podido calcular utilizando Word2Vec:\n",
    "\n",
    "<img src=\"word2vec6.png\"/>\n",
    "\n",
    "<img src=\"word2vec7.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos utilizar la adición de vectores para hacer preguntas como \"German + airlines\" y mirando las palabras más cercanas al vector compuesto obtenemos respuestas impresionantes:\n",
    "\n",
    "<img src=\"word2vec8.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir una representación vectorial de una palabra _y_ considerando las palabras X que aparacen frecuentemente en su contexto\n",
    "\n",
    "Para construir sus vectores, Word2Vec utiliza un dataset de entrenamiento y algoritmos de aprendizaje basados en redes neuronales (__Continuous Bag of Words__ (CBOW), o modelo __Skip Gram__). El objetivo de esta fase de aprendizaje es aprender cuáles son las palabras _X_ más probables de aparecer en el contexto de una palabra _y_.\n",
    "\n",
    "<img src=\"word2vec5.png\"/>\n",
    "\n",
    "Por ejemplo, ¿cuál es la probabilidad de tener la palabra 'perro' si aparece la palabra 'pelota' en el contexto?\n",
    "\n",
    "<code>Los expertos explican que los __perros__ persiguen __pelotas__ en movimiento como parte de un comportamiento instintivo. Aunque no todos los perros tienen tan despiertos su instinto de caza, esto no impide que la mayoría de ellos sí disfruten, y mucho, de los juegos que incluyen persecuciones de una saltarina __pelota__ que bota delante de ellos. </code>\n",
    "\n",
    "__Algoritmo CBOW__\n",
    "\n",
    "Las palabras de contexto forman la capa de entrada. Si el tamaño del vocabulario es V, estos serán vectores de dimensión V con sólo uno de los elementos establecido en uno, y el resto todos los ceros. Hay una sola capa oculta y una capa de salida.\n",
    "\n",
    "<img src=\"word2vec9.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec: crear representaciones vectoriales de palabras\n",
    "\n",
    "La clase <code>word2vec</code> de Gensim permite entrenar modelos vectoriales de palabras (ver documentación: https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "Esta clase tiene varios parametros, en particular:\n",
    "- <code>sentences</code>: una lista de palabras o de frases que sirve para entrenar el modelo\n",
    "- <code>sg</code>: define que algoritmos de aprendizaje utilizar (0=CBOW, 1=skip-gram)\n",
    "- <code>size</code>: define la dimensión de los vectores que se desea extraer\n",
    "- <code>window</code>: define el número de palabras considerar a la izquierda y a la derecha de una palabra\n",
    "- <code>min_count</code>: ignorar las palabras que aparecen menos de _min_count_\n",
    "y otros asociados a la parametrización de la fase de aprendizaje de la red neuronal (que no detallaremos en esta parte del curso):\n",
    "- <code>alpha</code>: el _learning rate_ utilizado para optimizar los parametros de la red neuronal.\n",
    "- <code>iter</code>: número de iteraciones (epocas) sobre el dataset para encontrar los parametreos que optimizan la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar nuestro modelo Word2Vec, podemos utilizar nuestros propios datasets o utilizar datasets genericos existentes. Para empezar, utilizaremos 100 MB de textos extraidos de Wikipedia en inglés, para generar vectores de 200 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = word2vec.Text8Corpus('text8.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences,size=200,hs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=71290, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos aprendido nuestro modelo, tratemos de resolver la ecuación <code>King - Man + Woman</code>.\n",
    "\n",
    "En otras palabras buscamos cuál es el vector más similar al vector que adiciona positivamente 'King' y 'Woman' y negativamente 'Man'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.5484005212783813),\n",
       " ('throne', 0.5342251658439636),\n",
       " ('princess', 0.5262429118156433),\n",
       " ('prince', 0.5059376955032349),\n",
       " ('sibylla', 0.4873780608177185)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman','king'],negative=['man'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conflicts', 0.7188023328781128),\n",
       " ('clashes', 0.6643460988998413),\n",
       " ('confrontation', 0.6346426606178284),\n",
       " ('struggle', 0.6321094632148743),\n",
       " ('hostilities', 0.6279382109642029),\n",
       " ('dispute', 0.6173650622367859),\n",
       " ('tensions', 0.6133891344070435),\n",
       " ('disputes', 0.5968484878540039),\n",
       " ('strife', 0.5911890864372253),\n",
       " ('disagreements', 0.5742663145065308)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"conflict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('confrontation', 0.5866920948028564),\n",
       " ('warfare', 0.569470226764679),\n",
       " ('weapons', 0.5322331786155701),\n",
       " ('fighting', 0.5214237570762634),\n",
       " ('conflicts', 0.5172784328460693),\n",
       " ('struggle', 0.5165830850601196),\n",
       " ('warheads', 0.5029329061508179),\n",
       " ('rifle', 0.4830498695373535),\n",
       " ('combat', 0.480122834444046),\n",
       " ('firearm', 0.46499496698379517)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"conflict\",\"weapon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clashes', 0.5157795548439026),\n",
       " ('disagreements', 0.5025807023048401),\n",
       " ('conflicts', 0.5005526542663574),\n",
       " ('disputes', 0.45643964409828186),\n",
       " ('dispute', 0.4438190162181854),\n",
       " ('strife', 0.4340599775314331),\n",
       " ('tensions', 0.43264490365982056),\n",
       " ('hostilities', 0.42931511998176575),\n",
       " ('reconciling', 0.4135684370994568),\n",
       " ('reconciliation', 0.4083474278450012)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"conflict\"],negative=[\"weapon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('childhood', 0.5315465331077576),\n",
       " ('lives', 0.5219142436981201),\n",
       " ('career', 0.4918401837348938),\n",
       " ('experiences', 0.46913063526153564),\n",
       " ('adolescence', 0.46442317962646484),\n",
       " ('humanity', 0.444394052028656),\n",
       " ('teens', 0.43493372201919556),\n",
       " ('universe', 0.4321097731590271),\n",
       " ('work', 0.42127031087875366),\n",
       " ('autobiography', 0.41469377279281616)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"life\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('passionate', 0.5382745265960693),\n",
       " ('happiness', 0.5342760682106018),\n",
       " ('affection', 0.5294280052185059),\n",
       " ('loving', 0.5253655910491943),\n",
       " ('dreams', 0.5175389051437378),\n",
       " ('dream', 0.49790388345718384),\n",
       " ('passion', 0.4970376789569855),\n",
       " ('devotion', 0.49456143379211426),\n",
       " ('humanity', 0.48934948444366455),\n",
       " ('enthusiasm', 0.48544085025787354)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"life\",\"love\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('profits', 0.5381085872650146),\n",
       " ('happiness', 0.4808928966522217),\n",
       " ('estate', 0.46329301595687866),\n",
       " ('earnings', 0.453243225812912),\n",
       " ('debts', 0.44948142766952515),\n",
       " ('donations', 0.44741061329841614),\n",
       " ('funds', 0.4426189661026001),\n",
       " ('investments', 0.43472820520401),\n",
       " ('credit', 0.43346893787384033),\n",
       " ('allowance', 0.4328705668449402)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"life\",\"money\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('humanity', 0.6343278884887695),\n",
       " ('humankind', 0.5613822937011719),\n",
       " ('dignity', 0.5261437296867371),\n",
       " ('selfishness', 0.5177077054977417),\n",
       " ('mankind', 0.5122897624969482),\n",
       " ('lives', 0.508557140827179),\n",
       " ('immortality', 0.5009029507637024),\n",
       " ('salvation', 0.49758243560791016),\n",
       " ('desires', 0.49654945731163025),\n",
       " ('pleasure', 0.4888823926448822)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"life\",\"happiness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universe', 0.3613162040710449),\n",
       " ('poem', 0.3577354848384857),\n",
       " ('narrative', 0.3437854051589966),\n",
       " ('satires', 0.33983439207077026),\n",
       " ('genius', 0.33316218852996826),\n",
       " ('incarnations', 0.32065248489379883),\n",
       " ('depiction', 0.3203204870223999),\n",
       " ('moments', 0.3182471692562103),\n",
       " ('prose', 0.31105563044548035),\n",
       " ('autobiography', 0.31091123819351196)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"life\"],negative=[\"health\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver los parametros aprendidos por la red neuronal para una palabra dada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.69125611e-01,  3.67443919e-01, -7.01698601e-01, -5.57390809e-01,\n",
       "        1.83433318e+00,  6.62127137e-01,  1.33304608e+00,  7.65880108e-01,\n",
       "        1.18038788e-01,  1.44866943e+00, -1.67192280e-01, -3.01816076e-01,\n",
       "       -1.28658462e+00,  2.16931272e+00, -6.68421566e-01,  6.73904121e-01,\n",
       "       -1.00405216e-01,  1.45309901e+00,  1.68849874e+00, -4.35919017e-01,\n",
       "        3.78655791e-01, -8.41010153e-01,  2.67007500e-01,  1.83593726e+00,\n",
       "        5.54438949e-01,  1.07396770e+00,  7.07256794e-01, -2.04979992e+00,\n",
       "        7.08761871e-01,  4.38214898e-01, -5.52625299e-01, -1.70860991e-01,\n",
       "       -4.44031149e-01, -1.15913165e+00, -3.90957862e-01,  1.16910565e+00,\n",
       "        4.06878680e-01,  6.74331009e-01, -2.59698648e-02,  1.31449342e+00,\n",
       "        2.22232997e-01,  7.49504939e-02,  8.72514069e-01, -1.45918298e+00,\n",
       "        9.43113685e-01,  8.56491685e-01, -2.58627117e-01, -1.70304179e-01,\n",
       "       -6.51856959e-02,  5.47520667e-02,  1.56213284e-01,  2.66357422e+00,\n",
       "        7.10851729e-01, -4.50323015e-01, -4.80090439e-01,  6.68221414e-01,\n",
       "       -4.06989902e-01, -1.81622303e+00, -1.80055067e-01, -1.60243714e+00,\n",
       "        3.25557619e-01,  3.11495692e-01,  1.08852482e+00, -1.18999049e-01,\n",
       "       -3.58268589e-01, -1.39166021e+00, -1.32977188e+00, -6.10640526e-01,\n",
       "        1.05415277e-01,  4.14228052e-01, -1.08155429e-01, -6.87305748e-01,\n",
       "        6.52885199e-01, -2.89527953e-01,  1.04591501e+00,  6.21275842e-01,\n",
       "       -1.02410626e+00, -7.23914653e-02,  7.39182755e-02, -1.04246151e+00,\n",
       "        1.13139816e-01, -2.51486301e-01,  6.10496283e-01, -1.98582813e-01,\n",
       "       -1.34314466e+00,  3.53089005e-01,  8.18375945e-01,  7.61478841e-01,\n",
       "        1.46407700e+00, -7.75316358e-01,  8.29386413e-01, -4.05643970e-01,\n",
       "       -6.53185621e-02,  2.78354019e-01,  1.24838006e+00, -6.99101150e-01,\n",
       "        2.39206478e-02,  2.53158480e-01, -4.50198889e-01,  8.95812988e-01,\n",
       "        5.05177200e-01,  1.28646657e-01, -2.52358943e-01, -1.91880405e+00,\n",
       "       -9.09724593e-01, -1.43747121e-01,  2.20580220e+00, -5.71050346e-01,\n",
       "        1.11589238e-01, -6.67722762e-01,  2.03356460e-01, -5.85539520e-01,\n",
       "        1.57739806e+00, -1.26681709e+00, -8.93747866e-01, -2.14932871e+00,\n",
       "       -1.64085869e-02,  1.22115672e+00,  6.97116256e-01,  7.90023744e-01,\n",
       "       -1.26370370e+00,  1.92026079e-01, -3.09529096e-01,  1.81705844e+00,\n",
       "       -1.23478584e-01, -1.62916616e-01, -1.87054563e+00,  1.13843298e+00,\n",
       "        9.31263566e-01,  2.83096172e-02,  2.23344415e-01,  4.33259815e-01,\n",
       "        6.07846260e-01, -1.23490304e-01,  9.07002687e-01, -5.49205363e-01,\n",
       "        7.64282882e-01,  1.11685717e+00, -1.54411662e+00, -1.06131899e+00,\n",
       "       -4.90093380e-01, -6.33298993e-01,  2.53341645e-01,  7.40984559e-01,\n",
       "        1.12368250e+00,  9.06969249e-01,  1.85874589e-02,  1.88795909e-01,\n",
       "       -2.13812129e-03, -8.73375297e-01,  1.45737886e+00, -3.54401618e-02,\n",
       "       -1.04208934e+00,  1.15996040e-01, -1.75689399e+00,  1.41484165e+00,\n",
       "       -1.47168294e-01,  3.50898713e-01,  1.18056774e+00, -1.21205889e-01,\n",
       "        8.40423882e-01,  1.09564984e+00,  4.15463805e-01,  1.66911912e+00,\n",
       "       -3.21701691e-02,  5.45190632e-01, -5.13920665e-01, -5.23721933e-01,\n",
       "        1.53825021e+00, -6.01137698e-01, -1.00290021e-02,  2.09836984e+00,\n",
       "        1.86220467e+00, -2.91116744e-01,  8.00611079e-01,  9.96657491e-01,\n",
       "        9.48190391e-01, -3.13537791e-02, -6.86806798e-01, -9.81293499e-01,\n",
       "       -7.60279596e-01, -1.02922535e-02, -2.02216461e-01,  3.21515769e-01,\n",
       "       -1.06946826e+00,  2.63011485e-01, -2.69687355e-01,  1.87627041e+00,\n",
       "        9.68080878e-01,  4.12665099e-01, -9.82611537e-01, -4.66148943e-01,\n",
       "        5.97405791e-01, -4.66996312e-01, -7.89638221e-01, -6.25219226e-01,\n",
       "       -8.93179774e-01,  9.10954714e-01, -4.20437932e-01,  5.25658876e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"text8_model\")\n",
    "model=word2vec.Word2Vec.load(\"text8_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro ejemplo de aplicación: __buscar el vector más diferente de los otros__.\n",
    "\n",
    "Entre las palabras siguientes, ¿cuál es la palabra la más distinta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvernier/.local/lib/python3.5/site-packages/gensim/models/keyedvectors.py:876: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"brazil chile france peru argentina\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hammer'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"apple pear banana hammer\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos tambien medir la similaridad entre palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7032394"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24828681"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','hammer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15005152"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman','hammer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09911914"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0652638"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman','engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32168412"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','baby')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47237882"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman','baby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo aprendido con la red neuronal captura el significado de las palabras, pero también los sesgos humanos.\n",
    "\n",
    "\n",
    "Dado que los vectores obtenidos representan mejor el sentido de las palabras que las palabras mismas, sería interesante representar los documentos (textos, moticias) utilizando estos vectores. Es la idea del algoritmo __Doc2Vec__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Doc2Vec: Representación vectorial de los documentos utilizando _Word embeddings_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos lo importante que es la representación vectorial de los documentos, en todo tipo de tareas, por ejemplo de clasificación supervisada (_sentiment analysis_, etc.) o de _topic modeling_.\n",
    "\n",
    "Por cada una de estas tareas, tenemos que representar nuestros documentos como vectores. Hasta ahora, hemos visto representaciones vectoriales simples basados en modelos Bag-of-Words (BOW) o TF-IDF.\n",
    "\n",
    "Basándose en Word2Vec, los investigadores en informática también han implementado en los últimos años una representación vectorial de documentos, popularmente llamada __Doc2Vec__.\tEsto significa que ahora podemos usar el poder de la comprensión semántica de Word2Vec para describir documentos también.\n",
    "\n",
    "Para una presentación del funcionamiento interno de Doc2Vec, se puede leer el artículo de blog siguiente: https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "En nuestro caso, por el momento, utilizaremos Doc2Vec como una caja negra para generar representaciones vectoriales de los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ponernos en marcha, necesitaremos un conjunto de documentos para entrenar a nuestro modelo doc2vec. En teoría, un documento puede ser cualquier cosa, desde un breve tweet de 140 caracteres, un solo párrafo (por ejemplo, un resumen de un artículo de una revista), un artículo de noticias o un libro. En Tratamiento Automáticamente del Lenguaje, una colección o conjunto de documentos a menudo se denomina corpus.\n",
    "\n",
    "Para este tutorial, vamos a entrenar nuestro modelo usando el Lee Background Corpus incluido en gensim. Este corpus contiene 314 documentos seleccionados del servicio de correo de noticias de la Corporación Australiana de Radiodifusión, que proporciona correos electrónicos de texto con titulares y cubre una serie de temas generales.\n",
    "\n",
    "Y probaremos nuestro modelo a ojo usando una muestra específica del Lee Corpus mucho más corto que contiene 50 documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_dir ='{}'.format(os.sep).join([gensim.__path__[0],'test','test_data'])\n",
    "lee_train_file=test_data_dir+os.sep+'lee_background.cor'\n",
    "lee_test_file=test_data_dir+os.sep+'lee.cor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Definir una función para leer y preprocesar el texto__\n",
    "\n",
    "A continuación, definimos una función para abrir el archivo de training y test (con codificación latina), leer el archivo línea por línea, preprocesar cada línea usando una simple herramienta de preprocesamiento de gensim (es decir, tokenizar texto en palabras individuales, eliminar la puntuación, ponerlo en minúsculas, etc.), y devolver una lista de palabras.\n",
    "\n",
    "Tenga en cuenta que, para un archivo determinado, cada línea constituye un único documento y la longitud de cada línea (es decir, documento) puede variar. Además, para entrenar el modelo, necesitaremos asociar una etiqueta/número a cada documento del corpus de entrenamiento. En nuestro caso, la etiqueta es simplemente el número de línea basado en cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(file_name,tokens_only=False):\n",
    "    with smart_open.smart_open(file_name,encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line),[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file,tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miremos el resultado del preprocesamiento, observando los 2 primeros documentos del corpus de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]),\n",
       " TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y el dataset de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'national',\n",
       "  'executive',\n",
       "  'of',\n",
       "  'the',\n",
       "  'strife',\n",
       "  'torn',\n",
       "  'democrats',\n",
       "  'last',\n",
       "  'night',\n",
       "  'appointed',\n",
       "  'little',\n",
       "  'known',\n",
       "  'west',\n",
       "  'australian',\n",
       "  'senator',\n",
       "  'brian',\n",
       "  'greig',\n",
       "  'as',\n",
       "  'interim',\n",
       "  'leader',\n",
       "  'shock',\n",
       "  'move',\n",
       "  'likely',\n",
       "  'to',\n",
       "  'provoke',\n",
       "  'further',\n",
       "  'conflict',\n",
       "  'between',\n",
       "  'the',\n",
       "  'party',\n",
       "  'senators',\n",
       "  'and',\n",
       "  'its',\n",
       "  'organisation',\n",
       "  'in',\n",
       "  'move',\n",
       "  'to',\n",
       "  'reassert',\n",
       "  'control',\n",
       "  'over',\n",
       "  'the',\n",
       "  'party',\n",
       "  'seven',\n",
       "  'senators',\n",
       "  'the',\n",
       "  'national',\n",
       "  'executive',\n",
       "  'last',\n",
       "  'night',\n",
       "  'rejected',\n",
       "  'aden',\n",
       "  'ridgeway',\n",
       "  'bid',\n",
       "  'to',\n",
       "  'become',\n",
       "  'interim',\n",
       "  'leader',\n",
       "  'in',\n",
       "  'favour',\n",
       "  'of',\n",
       "  'senator',\n",
       "  'greig',\n",
       "  'supporter',\n",
       "  'of',\n",
       "  'deposed',\n",
       "  'leader',\n",
       "  'natasha',\n",
       "  'stott',\n",
       "  'despoja',\n",
       "  'and',\n",
       "  'an',\n",
       "  'outspoken',\n",
       "  'gay',\n",
       "  'rights',\n",
       "  'activist'],\n",
       " ['cash',\n",
       "  'strapped',\n",
       "  'financial',\n",
       "  'services',\n",
       "  'group',\n",
       "  'amp',\n",
       "  'has',\n",
       "  'shelved',\n",
       "  'million',\n",
       "  'plan',\n",
       "  'to',\n",
       "  'buy',\n",
       "  'shares',\n",
       "  'back',\n",
       "  'from',\n",
       "  'investors',\n",
       "  'and',\n",
       "  'will',\n",
       "  'raise',\n",
       "  'million',\n",
       "  'in',\n",
       "  'fresh',\n",
       "  'capital',\n",
       "  'after',\n",
       "  'profits',\n",
       "  'crashed',\n",
       "  'in',\n",
       "  'the',\n",
       "  'six',\n",
       "  'months',\n",
       "  'to',\n",
       "  'june',\n",
       "  'chief',\n",
       "  'executive',\n",
       "  'paul',\n",
       "  'batchelor',\n",
       "  'said',\n",
       "  'the',\n",
       "  'result',\n",
       "  'was',\n",
       "  'solid',\n",
       "  'in',\n",
       "  'what',\n",
       "  'he',\n",
       "  'described',\n",
       "  'as',\n",
       "  'the',\n",
       "  'worst',\n",
       "  'conditions',\n",
       "  'for',\n",
       "  'stock',\n",
       "  'markets',\n",
       "  'in',\n",
       "  'years',\n",
       "  'amp',\n",
       "  'half',\n",
       "  'year',\n",
       "  'profit',\n",
       "  'sank',\n",
       "  'per',\n",
       "  'cent',\n",
       "  'to',\n",
       "  'million',\n",
       "  'or',\n",
       "  'share',\n",
       "  'as',\n",
       "  'australia',\n",
       "  'largest',\n",
       "  'investor',\n",
       "  'and',\n",
       "  'fund',\n",
       "  'manager',\n",
       "  'failed',\n",
       "  'to',\n",
       "  'hit',\n",
       "  'projected',\n",
       "  'per',\n",
       "  'cent',\n",
       "  'earnings',\n",
       "  'growth',\n",
       "  'targets',\n",
       "  'and',\n",
       "  'was',\n",
       "  'battered',\n",
       "  'by',\n",
       "  'falling',\n",
       "  'returns',\n",
       "  'on',\n",
       "  'share',\n",
       "  'markets']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Entrenamiento del modelo: Instanciación de un objeto Doc2Vec__\n",
    "\n",
    "Ahora, instanciaremos un modelo de Doc2Vec con un tamaño vectorial de 50 palabras e iterando sobre el corpus de entrenamiento 40 veces. Fijamos el número mínimo de palabras en 2 para poder descartar palabras con muy pocas ocurrencias (sin una variedad de ejemplos representativos, retener palabras tan poco frecuentes puede empeorar el modelo).\n",
    "\n",
    "Noten que se trata de un conjunto de datos muy pequeño (300 documentos) con documentos más cortos (unos pocos cientos de palabras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvernier/.local/lib/python3.5/site-packages/gensim/models/doc2vec.py:571: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/home/mvernier/.local/lib/python3.5/site-packages/gensim/models/doc2vec.py:575: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2, iter=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Construir el vocabulario__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipicamente, el vocabulario es un diccionario (accesible a través de <code>model.wv.vocabulario</code>) de todas las palabras únicas extraídas del corpus de entrenamiento junto con el recuento.\n",
    "\n",
    "Por ejemplo, model.wv.vocab['penalty'].count para recuentos para la palabra 'penalty'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab['penalty'].count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Entrenar el modelo Doc2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inferir un vector__\n",
    "\n",
    "Una cosa importante a tener en cuenta es que ahora puede inferir un vector para cualquier pieza de texto sin tener que volver a entrenar al modelo pasando una lista de palabras a la función model.infer_vector. Este vector puede entonces compararse con otros vectores a través de la similitud del coseno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15396346, -0.09650686, -0.05873749, -0.07886393,  0.1358658 ,\n",
       "        0.1447225 , -0.21712308,  0.05572385,  0.07102157, -0.04276712,\n",
       "       -0.04617002,  0.00292877,  0.03713769,  0.04658063,  0.08701105,\n",
       "        0.08587693,  0.08586249, -0.06595804, -0.01872166,  0.14572152,\n",
       "       -0.02078765,  0.12997988,  0.06471211,  0.04577038,  0.13596691,\n",
       "        0.050091  ,  0.09107733, -0.0275902 , -0.11446794, -0.05491024,\n",
       "       -0.14875777, -0.02547036, -0.03163017, -0.3010016 ,  0.11017639,\n",
       "       -0.14555633, -0.02939347, -0.15385276,  0.09029843,  0.00492589,\n",
       "       -0.12010241, -0.11402632,  0.1957001 , -0.06019856, -0.16582663,\n",
       "       -0.01470555, -0.02464065,  0.16318351,  0.03915374, -0.25612864],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forrest', 'fires'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que <code>infer_vector()</code> no toma una String, sino más bien una lista de tokens, que ya deberían haber sido tokenizados de la misma manera que las palabras propiedad de los objetos originales del documento de formación.\n",
    "\n",
    "También tenga en cuenta que debido a que los algoritmos de entrenamiento son un problema de aproximación iterativa que hace uso de la aleatorización interna, las inferencias repetidas del mismo texto devolverán vectores ligeramente diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Validación del modelo__\n",
    "\n",
    "Para evaluar nuestro nuevo modelo, primero inferiremos nuevos vectores para cada documento del corpus de entrenamiento, compararemos los vectores inferidos con el corpus de entrenamiento, y luego devolveremos el rango del documento basado en la autosimilaridad. \n",
    "\n",
    "Básicamente, estamos suponiendo que el corpus de entrenamiento es un dato nuevo e invisible y luego vemos cómo se compara con el modelo entrenado. La expectativa es que probablemente hemos adaptado demasiado nuestro modelo (es decir, todos los rangos serán inferiores a 2) y por lo tanto deberíamos ser capaces de encontrar documentos similares muy fácilmente. Además, realizaremos un seguimiento de las segundas posiciones para comparar documentos menos similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contemos cómo se clasifica cada documento con respecto al corpus de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 292, 1: 8})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(ranks)  # Results vary between runs due to random seeding and very small corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Básicamente, más del 95% de los documentos inferidos son más similares a sí mismos y alrededor del 5% de las veces es erróneamente más similar a otro documento. La comprobación de un vector inferido contra un vector entrenado es una especie de control sobre si el modelo se está comportando de una manera útil y coherente, aunque no un valor real de \"exactitud\".\n",
    "\n",
    "Podemos echar un vistazo a un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (299): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (0, 0.947723388671875): «hundreds of people have been forced to vacate their homes in the southern highlands of new south wales as strong winds today pushed huge bushfire towards the town of hill top new blaze near goulburn south west of sydney has forced the closure of the hume highway at about pm aedt marked deterioration in the weather as storm cell moved east across the blue mountains forced authorities to make decision to evacuate people from homes in outlying streets at hill top in the new south wales southern highlands an estimated residents have left their homes for nearby mittagong the new south wales rural fire service says the weather conditions which caused the fire to burn in finger formation have now eased and about fire units in and around hill top are optimistic of defending all properties as more than blazes burn on new year eve in new south wales fire crews have been called to new fire at gunning south of goulburn while few details are available at this stage fire authorities says it has closed the hume highway in both directions meanwhile new fire in sydney west is no longer threatening properties in the cranebrook area rain has fallen in some parts of the illawarra sydney the hunter valley and the north coast but the bureau of meteorology claire richards says the rain has done little to ease any of the hundred fires still burning across the state the falls have been quite isolated in those areas and generally the falls have been less than about five millimetres she said in some places really not significant at all less than millimetre so there hasn been much relief as far as rain is concerned in fact they ve probably hampered the efforts of the firefighters more because of the wind gusts that are associated with those thunderstorms»\n",
      "\n",
      "MEDIAN (272, 0.816140353679657): «the storm clean up in sydney will resume in earnest this morning as fresh crews are brought in to replace state emergency service ses personnel who worked through the night the storm hit sydney early yesterday afternoon and two schoolgirls died when tree fell on them at reserve at hornsby heights in the city north number of other people were injured as the storm brought down trees and power poles and lifted roofs new south wales emergency services minister bob debus says welfare and emergency funding arrangements have been put in place with the declaration of natural disaster areas in campbeltown hornsby warringah and kurringai welfare services become available if they are needed local government is refunded any money it spends on the clean up or that it spends on repairing its own infrastructure low interest loans if they are needed are available to small business to help them get back on their feet again mr debus said energy australia says power has been restored to customers and work will continue today to reconnect those still without electricity energy australia peter leete says work will concentrate around the worst hit areas the worst of the problems we have still got are in sydney northern suburbs which seem to be the worst hit of all and that around hornsby st ives turramurra and frenches forest mr leete said four hundred ses volunteers are responding to more than calls for assistance the volunteers have worked throughout the night to remove trees from homes and roads the ses laura goodin says it will take several days before the damage is cleared up while the ses has received fewer calls for help than in the storm two weeks ago many of the jobs in this storm are actually quite complicated involving large trees or extensively damaged homes and businesses we re estimating that most of the tasks will be completed by friday if no new storms develop ms goodin said outside sydney the storms caused damage in north east of the state and the lower hunter scores of homes and farm buildings have been damaged and literally hundreds of trees have been brought down the storms accompanied by gale force winds and hail left large areas around tamworth gunnedah and quirindi without electricity and telephone services»\n",
      "\n",
      "LEAST (105, 0.7206535935401917): «fresh palls of smoke are billowing from the woomera detention centre in south australia far north trouble at the centre has entered day three with plume of smoke metres high into the air and up to metres across the compound this morning thirteen buildings were either destroyed or damaged by fire on monday night overnight fires and rioting appeared to have abated just after midnight local time three fire crews one ambulance and several police have attended the scene water cannon and three tear gas canisters were used to subdue detainees who throughout the night were thought to be chanting visa it is not known whether anyone has been injured or arrested overnight the acting immigration minister daryl williams says the government is not losing control of woomera he has told channel nine vandalism is not going to get visas for the detainees the detainees who have been provided with very good facilities and who to our knowledge have absolutely no complaint about the facilities there are engaging in this campaign of damaging and destroying buildings in order to put pressure on the australian authorities to grant them visa he said there is plea for so called high risk detainees to be separated from the rest of the population ath the woomera detention centre in the wake of continued disturbances there south australian labor mp lyn breuer whose electorate covers woomera says higher risk detainees must be separated from women and children at the centre think that will probably have to be the ultimate solution we will have to send high risk detainees to other areas she said we can keep them in an environment where there are young children there it all very nasty situation and have particular concerns for the people that are guarding them as well because one of them is going to get hurt very badly very soon»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note arriba que el documento más similar tiene una puntuación de similitud que se acerca a 1.0. Sin embargo, la puntuación de similitud para los documentos de segundo orden debería ser significativamente menor.\n",
    "\n",
    "Podemos ejecutar la siguiente celda repetidamente para ver una muestra de otras comparaciones de documento-objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (272): «the storm clean up in sydney will resume in earnest this morning as fresh crews are brought in to replace state emergency service ses personnel who worked through the night the storm hit sydney early yesterday afternoon and two schoolgirls died when tree fell on them at reserve at hornsby heights in the city north number of other people were injured as the storm brought down trees and power poles and lifted roofs new south wales emergency services minister bob debus says welfare and emergency funding arrangements have been put in place with the declaration of natural disaster areas in campbeltown hornsby warringah and kurringai welfare services become available if they are needed local government is refunded any money it spends on the clean up or that it spends on repairing its own infrastructure low interest loans if they are needed are available to small business to help them get back on their feet again mr debus said energy australia says power has been restored to customers and work will continue today to reconnect those still without electricity energy australia peter leete says work will concentrate around the worst hit areas the worst of the problems we have still got are in sydney northern suburbs which seem to be the worst hit of all and that around hornsby st ives turramurra and frenches forest mr leete said four hundred ses volunteers are responding to more than calls for assistance the volunteers have worked throughout the night to remove trees from homes and roads the ses laura goodin says it will take several days before the damage is cleared up while the ses has received fewer calls for help than in the storm two weeks ago many of the jobs in this storm are actually quite complicated involving large trees or extensively damaged homes and businesses we re estimating that most of the tasks will be completed by friday if no new storms develop ms goodin said outside sydney the storms caused damage in north east of the state and the lower hunter scores of homes and farm buildings have been damaged and literally hundreds of trees have been brought down the storms accompanied by gale force winds and hail left large areas around tamworth gunnedah and quirindi without electricity and telephone services»\n",
      "\n",
      "Similar Document (255, 0.859774112701416): «the new south wales state emergency service ses says it has now received calls for help in the wake of monday fierce storms natural disaster areas have been declared throughout sydney and surrounding areas and parts of the state north west in sydney more than homes mainly in the northern suburbs remain without power ses spokeswoman laura goodin says several hundred volunteers will be back in the field this morning we ve had about calls for help of which we ve completed about two thirds we ve had about volunteers in the field being helped out by the royal fire service and the new south wales fire brigades and we re expecting to have most jobs completed by about friday ms goodin said the extensive storm damage has prompted warning about people falsely claiming to work for the ses the warning from fair trading minister john aquilina follows reports from the suburb of hornsby that people claiming to work for the ses are asking for payment from the storm victims mr aquilina has reminded householders that the ses is volunteer organisation and does not charge for its work or employ sub contractors he has suggested residents contact the police if they are approached by such people the government is also warning householders against dealing with unlicensed tradespeople»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prueba del modelo__\n",
    "\n",
    "Usando el mismo enfoque anterior, inferiremos el vector para un documento de prueba elegido al azar, y compararemos el documento con nuestro modelo a ojo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (8): «hunan province remained on high alert last night as thunderstorms threatened to exacerbate the flood crisis now entering its fifth day and with already dead and hundreds of thousands evacuated on the flood frontline at dongting lake the water level peaked at just under on saturday night then eased about cm during the day under hot sun with temperatures reaching but with the lake still brimming at dangerously high levels and spilling over the top of its banks in some places locals were fearful that thunderstorm and high winds forecast to hit the region last night would damage the dikes about km of dikes around the lake are all that stand between million people in the surrounding farmland and disaster»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (77, 0.6922479867935181): «the woomera detention centre in outback south australia has experienced its first quiet night this week since sunday it seems the government decision to put freeze on the processing of visa applications is working from outside the centre it appeared there were no major incidents at the facility all day yesterday and no unrest overnight at about am acdt this morning minibus load of security officers left the centre and extra police resources brought into the town in recent days were stood down the immigration department claims the damage bill from the arson attacks and vandalism at the centre over the past month has reached million spokeswoman has rejected claims that detainees at the woomera centre are to be relocated to port augusta el alamein centre next month saying there is room for another people at woomera»\n",
      "\n",
      "MEDIAN (260, 0.2962673306465149): «traveland wholly owned travel centres have ceased operating from today leaving more than staff seeking other jobs the failed company administrators say they have buyer for traveland franchise network but have not been able to save the company stores one of the administrators richard albarran says the deal which is yet to be approved by committee formed today of creditors will unfortunately leave hundreds of people who have booked holidays through the company stores out of pocket the dollar value is approximately tad over million mr albarran said they will now be entitled to make claims through the travel compensation fund the meeting of company creditors was told this morning staff are owed nearly million in entitlements the australian services union luke foley says he will be doing everything he can to ensure they receive every cent ansett administrators are liable for perhaps the lion share of those employee entitlements we re confident that ll be met mr foley said and more than staff are to lose their jobs at australia biggest regional pay television operator austar the company has this morning announced wide ranging restructuring plans management at the struggling pay tv operator has now completed review of all its activities as result of this review the austar board has decided to outsource number of existing functions cease operating its own internet network and streamline other processes the company anticipates annualised savings of around million more than staff will be made redundant from the end of december austar has given assurances that redundant workers will receive their full entitlements and redundancy payments in line with company policy the company says they will receive all statutory entitlements and redundancy payments in line with company policy on the stock exchange austar shares rose five cents to cents shortly before pm aedt»\n",
      "\n",
      "LEAST (131, -0.05119406059384346): «the new solomon islands prime minister has told his people that there are tough times ahead sir allan kemakeza the parliamentary leader of the people alliance party was elected prime minister on the first ballot sir allan heads up team consisting of the surviving members of the outgoing government and large grouping of newly elected independents one of those held one of the most senior positions in the malaita eagle force militia that conducted last year coup while others who have backed him were elected after being endorsed by the rival guadalcanal militia the isatabu freedom movement that ethnic conflict has left the solomons economy close to collapse and hundreds of high powered guns remain with the militants sir allan is putting his trust in god the times ahead are not going to be easy he said these will be times of sacrifice sir allan says that he also hopes to get agreement on disarming rival militias in the country within his first days in office in an attempt to remove any remaining high powered weapons from the community he believes it was his contacts with the grass root militias on both sides of the ethnic war that led to the success of the townsville peace conference that brought the war to halt sir allan former policeman says the police force part of which took part in last year coup needs to be overhauled both must go together the disarmament program as well as the restructuring of the police force he said he says law and order will be one of his government priorities palestinian leader yasser arafat call for an end to attacks on israelis has been met with mixture of hope scepticism and defiance under enormous international pressure to halt the violence mr arafat has called for halt to all armed operations against israel including suicide bombings and vowed that the perpetrators would be punished france britain and the united states have welcomed the announcement the israeli cabinet minister ephraim sneh says while it is positive sign the palestinian authority has to act on its words if he proves that he really means to act very very forcefully sincerely effectively and seriously against the islamic jihad and hamas and his own tanzin movement that will be positive sign he said»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Ejercicios - TP\n",
    "\n",
    "1. Entrenar un modelo Word2Vec para el español. Se puede utilizar agrupar datasets de \"Sophia\" en un solo dataset grande: \n",
    "\n",
    "2. Entrenar un modelo Doc2Vec para el español.\n",
    "\n",
    "3. Utilizar representaciones Doc2Vec para volver a resolver tareas de clasificación y/o de topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
